{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8620fd59",
   "metadata": {},
   "source": [
    "### Load in LM for testing\n",
    "- interstingly gpt2-medium throws nan's but script seems to work well for small and large\n",
    "- each lm_head has different dims so small is 768, medium 1024, large 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1c0835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cache_dir = (Path.cwd() / \"models\").resolve()\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    # else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a96d53",
   "metadata": {},
   "source": [
    "### Change basis for lm_head\n",
    "- [linear_rep_geometry\n",
    "/store_matrices.py](https://github.com/KihoPark/linear_rep_geometry/blob/main/store_matrices.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05d815b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "torch.Size([50257, 1280])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.lm_head.weight.detach()\n",
    "W, d = gamma.shape\n",
    "gamma_bar = torch.mean(gamma, dim=0)\n",
    "centered_gamma = gamma - gamma_bar\n",
    "\n",
    "### compute Cov(gamma) and tranform gamma to g ###\n",
    "cov_gamma = centered_gamma.T @ centered_gamma / W\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_gamma)\n",
    "\n",
    "inv_sqrt_cov_gamma = eigenvectors @ torch.diag(1/torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "sqrt_cov_gamma = eigenvectors @ torch.diag(torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# gamma is our original head and inv_sqrt_cov_gamma puts us in a causal basis\n",
    "g = gamma @ inv_sqrt_cov_gamma\n",
    "\n",
    "# maybe i confused but A_inv = sqrt_cov_gamma and A = inv_sqrt_cov_gamma for \n",
    "# l(x).T @ g(y)\n",
    "# where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A (referencing paper eq and presentation eq on youtube)\n",
    "print(model.config.hidden_size)\n",
    "print(g.size())\n",
    "\n",
    "# g = g.float()\n",
    "# inv_sqrt_cov_gamma = inv_sqrt_cov_gamma.float()\n",
    "# sqrt_cov_gamma = sqrt_cov_gamma.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5975acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenval min: 7.621865370310843e-05\n",
      "Eigenval max: 0.08969025313854218\n",
      "Max amplification (1/sqrt(min)): 114.5\n",
      "\n",
      "gamma min: -0.5182147026062012\n",
      "gamma max: 0.4273260533809662\n",
      "\n",
      "gamma @ inv_sqrt_cov_gamma min: -8.831315994262695\n",
      "gamma @ inv_sqrt_cov_gamma max: 8.074169158935547\n",
      "\n",
      "gamma dtype: torch.float32\n",
      "g dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "eigenval_min_max = f\"Eigenval min: {eigenvalues.min()}\\nEigenval max: {eigenvalues.max()}\"\n",
    "max_amp = f\"Max amplification (1/sqrt(min)): {1 / torch.sqrt(eigenvalues.min()).item():.1f}\\n\"\n",
    "gamma_min_max = f\"gamma min: {gamma.min()}\\ngamma max: {gamma.max()}\\n\"\n",
    "g_min_max = f\"gamma @ inv_sqrt_cov_gamma min: {g.min()}\\ngamma @ inv_sqrt_cov_gamma max: {g.max()}\\n\"\n",
    "\n",
    "print(eigenval_min_max)\n",
    "print(max_amp)\n",
    "print(gamma_min_max)\n",
    "print(g_min_max)\n",
    "print(f\"gamma dtype: {gamma.dtype}\")\n",
    "print(f\"g dtype: {g.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dfd26",
   "metadata": {},
   "source": [
    "### Define data for training probe and inference tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d11b2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "concept_df = pd.read_json('gendered_pairs/data.json')\n",
    "concept_df = concept_df[concept_df.get('skip', False) != True] # skipping pronoun contrastive pairs\n",
    "male_ex = concept_df['male_sentence'].to_list()\n",
    "female_ex = concept_df['female_sentence'].to_list()\n",
    "\n",
    "inference_prompts = [\n",
    "    \"Long live the\",\n",
    "    \"The lion is the\",\n",
    "    \"In the hierarchy of medieval society, the highest rank was the\",\n",
    "    \"Arthur was a legendary\",\n",
    "    \"He was known as the warrior\",\n",
    "    \"In a monarchy, the ruler is usually a\",\n",
    "    \"He sat on the throne, the\",\n",
    "    \"A sovereign ruler in a monarchy is often a\",\n",
    "    \"His domain was vast, for he was a\",\n",
    "    \"The lion, in many cultures, is considered the\",\n",
    "    \"He wore a crown, signifying he was the\",\n",
    "    \"A male sovereign who reigns over a kingdom is a\",\n",
    "    \"Every kingdom has its ruler, typically a\",\n",
    "    \"The prince matured and eventually became the\",\n",
    "    \"In the deck of cards, alongside the queen is the\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba974c9f",
   "metadata": {},
   "source": [
    "### get embedding concept reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c951dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_inputs = tokenizer(male_ex, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "f_inputs = tokenizer(female_ex, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model(**m_inputs, output_hidden_states=True)\n",
    "    out2 = model(**f_inputs, output_hidden_states=True)\n",
    "    last_hidden_m, last_hidden_f = out1.hidden_states[-1], out2.hidden_states[-1]\n",
    "    # we want to pool embeddings for each token in sequence so we have full sequence representation\n",
    "    m_emb, f_emb = last_hidden_m.mean(dim=1), last_hidden_f.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf1282",
   "metadata": {},
   "source": [
    "### train probe\n",
    "- could probably just get difference vec would like to ask about this tbh\n",
    "- gut says probe is better because error/noise can be better quantified \n",
    "    - plus can determine alpha that acheives desired prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e54baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# make sure probe is trained in right representation space \n",
    "X = torch.cat([m_emb, f_emb], dim=0) @ sqrt_cov_gamma\n",
    "\n",
    "y = torch.cat([\n",
    "    torch.ones(len(male_ex)), \n",
    "    torch.zeros(len(female_ex))\n",
    "    ])\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    penalty=None,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "clf.fit(X.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "concept_dir = torch.tensor(clf.coef_[0], dtype=torch.float32, device=device)\n",
    "concept_dir /= concept_dir.norm()\n",
    "\n",
    "# # simple difference\n",
    "# concept_dir = (m_emb - f_emb).mean(dim=0)\n",
    "# concept_dir = concept_dir @ sqrt_cov_gamma\n",
    "# concept_dir /= concept_dir.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be313a",
   "metadata": {},
   "source": [
    "### Inference steering\n",
    "- assuming i just keep adding gender dir at each decoding step\n",
    "- this could be simpler to put in logit processor so we can use model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ff19971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Long live the'\n",
      "\n",
      "| alpha = 1          | alpha = 0          | alpha = -1       |\n",
      "|--------------------|--------------------|------------------|\n",
      "| king (-1.35)       | king (-2.60)       | queen (-1.28)    |\n",
      "| King (-2.43)       | King (-3.22)       | Queen (-1.34)    |\n",
      "| Emperor (-3.17)    | Queen (-3.66)      | Goddess (-2.98)  |\n",
      "| emperor (-3.20)    | queen (-3.88)      | goddess (-3.74)  |\n",
      "| man (-4.33)        | new (-3.88)        | Empress (-4.18)  |\n",
      "| lord (-4.68)       | Emperor (-4.35)    | woman (-4.38)    |\n",
      "| Lord (-4.95)       | emperor (-4.62)    | women (-4.56)    |\n",
      "| son (-5.19)        | free (-4.76)       | feminist (-4.69) |\n",
      "| new (-5.19)        | revolution (-5.20) | mother (-4.79)   |\n",
      "| free (-5.20)       | Internet (-5.34)   | princess (-4.80) |\n",
      "| prince (-5.21)     | man (-5.39)        | new (-4.83)      |\n",
      "| revolution (-5.22) | people (-5.52)     | Princess (-4.96) |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "prompt = inference_prompts[0]\n",
    "prompt_enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "# what are sensible alphas here e.g. values that acheive some target prob for that class?\n",
    "alphas = [1, 0, -1]\n",
    "k = 12\n",
    "cols = []\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    with torch.no_grad():\n",
    "        # transform, steer, and get causal inner-product between embedding and unembedding\n",
    "        outputs = model(**prompt_enc, output_hidden_states=True)\n",
    "        last_token_idx = (prompt_enc.attention_mask.sum(dim=1) - 1).item()\n",
    "\n",
    "        # (l(x) + alpha * gender_norm).T @ g(y)\n",
    "        # where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A \n",
    "        # Note: Code seems to swap A_inv with A e.g. \n",
    "        # it applies A_inv to gamma (lm_head) instead of lambda (embeddings) like equation suggests\n",
    "\n",
    "        lambda_x = outputs.hidden_states[-1][:, last_token_idx, :] # last token emb but calling it lambda for paper consistence\n",
    "        l_causal = lambda_x @ sqrt_cov_gamma\n",
    "        l_steered = l_causal + alpha * concept_dir\n",
    "\n",
    "        causal_logits = l_steered @ g.T\n",
    "        causal_log_probs = torch.log_softmax(causal_logits, dim=-1)\n",
    "        topk = torch.topk(causal_log_probs, k)\n",
    "\n",
    "        tokens = [tokenizer.decode([idx.item()]) for idx in topk.indices[0]]\n",
    "        log_probs = topk.values[0].tolist()\n",
    "        cols.append([f\"{tok} ({lp:.2f})\" for tok, lp in zip(tokens, log_probs)])\n",
    "\n",
    "rows = list(zip(*cols))\n",
    "table = tabulate(rows, headers=[f\"alpha = {a}\" for a in alphas], tablefmt=\"github\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dce0c8",
   "metadata": {},
   "source": [
    "### custom lm_head \n",
    "if the code is right could easily create a logits processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b406ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "class SteerableGPT2(GPT2LMHeadModel):\n",
    "    def __init__(self, base_model, lm_head_g, sqrt_cov_gamma, concept_dir, alpha: float = 0.0):\n",
    "        super().__init__(base_model.config)\n",
    "        # reuse base model's transformer + original head\n",
    "        self.transformer = base_model.transformer\n",
    "        self.lm_head= base_model.lm_head\n",
    "\n",
    "        # g(y) = gamma(y) @ A where A = Cov(gamma)^(-1/2) \n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "\n",
    "        # A_inv = sqrt_cov_gamma = Cov(gamma)^(+1/2), used to map lambda -> l_causal\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "\n",
    "        # steering direction\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        # get all hidden states so we can grab the last layer\n",
    "        outputs = super().forward(*args, output_hidden_states=True, **kwargs)\n",
    "        lambda_all = outputs.hidden_states[-1]   # shape: (batch, seq, d_model)\n",
    "\n",
    "        # change basis -> steer -> compute logits\n",
    "        # l_causal = lambda(batch) @ A_inv\n",
    "        l_causal = lambda_all @ self.sqrt_cov_gamma\n",
    "\n",
    "        # steer only the last token: l_last = l_last + alpha * concept_dir\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + self.alpha * self.concept_dir\n",
    "\n",
    "        # logits = (l(x) + alpha * concept_dir).T @ g(y)\n",
    "        outputs.logits = l_causal @ self.lm_head_g.T\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)\n",
    "causal_model = SteerableGPT2(\n",
    "    base_model=base,\n",
    "    lm_head_g=g,\n",
    "    sqrt_cov_gamma=sqrt_cov_gamma,\n",
    "    concept_dir=concept_dir, \n",
    "    alpha=-1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2169bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:       The lion, in many cultures, is considered the\n",
      "Continuation: The lion, in many cultures, is considered the symbol of fertility. It was a goddess who had her breasts pierced and she herself menstruated for childbirths. She also wore\n"
     ]
    }
   ],
   "source": [
    "prompt = inference_prompts[9]\n",
    "prompt_enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "output = causal_model.generate(\n",
    "    input_ids=prompt_enc.input_ids,\n",
    "    attention_mask=prompt_enc.attention_mask,\n",
    "    max_new_tokens=25,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.85,\n",
    "    repetition_penalty=1.5,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"Prompt:       {prompt}\")\n",
    "print(f\"Continuation: {''.join(tokenizer.batch_decode(output))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18e9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
