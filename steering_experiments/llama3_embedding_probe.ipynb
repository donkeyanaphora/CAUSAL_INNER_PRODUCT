{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61731f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "# full GPU reset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cache_dir = (Path.cwd() / \"models\").resolve()\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    # else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "print(f'Device: {device}')\n",
    "model_card = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_card).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfeb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = model.lm_head.weight.detach()\n",
    "W, d = gamma.shape\n",
    "gamma_bar = torch.mean(gamma, dim=0)\n",
    "centered_gamma = gamma - gamma_bar\n",
    "\n",
    "### compute Cov(gamma) and tranform gamma to g ###\n",
    "cov_gamma = centered_gamma.T @ centered_gamma / W\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_gamma)\n",
    "\n",
    "inv_sqrt_cov_gamma = eigenvectors @ torch.diag(1/torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "sqrt_cov_gamma = eigenvectors @ torch.diag(torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# gamma is our original head and inv_sqrt_cov_gamma puts us in a causal basis\n",
    "g = gamma @ inv_sqrt_cov_gamma\n",
    "\n",
    "# maybe i confused but A_inv = sqrt_cov_gamma and A = inv_sqrt_cov_gamma for\n",
    "# l(x).T @ g(y)\n",
    "# where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A (referencing paper eq and presentation eq on youtube)\n",
    "print(model.config.hidden_size)\n",
    "print(g.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenval_min_max = f\"Eigenval min: {eigenvalues.min()}\\nEigenval max: {eigenvalues.max()}\"\n",
    "max_amp = f\"Max amplification (1/sqrt(min)): {1 / torch.sqrt(eigenvalues.min()).item():.1f}\\n\"\n",
    "gamma_min_max = f\"gamma min: {gamma.min()}\\ngamma max: {gamma.max()}\\n\"\n",
    "g_min_max = f\"gamma @ inv_sqrt_cov_gamma min: {g.min()}\\ngamma @ inv_sqrt_cov_gamma max: {g.max()}\\n\"\n",
    "\n",
    "print(eigenval_min_max)\n",
    "print(max_amp)\n",
    "print(gamma_min_max)\n",
    "print(g_min_max)\n",
    "print(f\"gamma dtype: {gamma.dtype}\")\n",
    "print(f\"g dtype: {g.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458eecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "concept_df = pd.read_json(\"https://raw.githubusercontent.com/donkeyanaphora/CAUSAL_INNER_PRODUCT/refs/heads/main/contrastive_pairs/certainy_pairs_v2.json\")\n",
    "concept_df = concept_df[concept_df.get('skip', False) != True]\n",
    "a = concept_df['certain_sentence'].to_list()\n",
    "b = concept_df['uncertain_sentence'].to_list()\n",
    "\n",
    "# concept_df = pd.read_json(\"https://raw.githubusercontent.com/donkeyanaphora/CAUSAL_INNER_PRODUCT/refs/heads/main/contrastive_pairs/counter_factuals.json\")\n",
    "# a = concept_df['factual_sentence'].to_list()\n",
    "# b = concept_df['counterfactual_sentence'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04146ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_inputs = tokenizer(a, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "b_inputs = tokenizer(b, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model(**a_inputs, output_hidden_states=True)\n",
    "    out2 = model(**b_inputs, output_hidden_states=True)\n",
    "    last_hidden_m, last_hidden_f = out1.hidden_states[-1], out2.hidden_states[-1]\n",
    "    # we want to pool embeddings for each token in sequence so we have full sequence representation\n",
    "    a_emb, b_emb = last_hidden_m.mean(dim=1), last_hidden_f.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dir = (a_emb - b_emb).mean(dim=0)\n",
    "concept_dir = concept_dir @ sqrt_cov_gamma\n",
    "concept_dir /= concept_dir.norm()\n",
    "concept_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "class SteerableLM(LlamaForCausalLM):\n",
    "    def __init__(self, base_model, lm_head_g, sqrt_cov_gamma, concept_dir, alpha: float = 0.0):\n",
    "        super().__init__(base_model.config)\n",
    "        # reuse base model's transformer + original head\n",
    "        self.model = base_model.model\n",
    "        self.lm_head= base_model.lm_head\n",
    "\n",
    "        # g(y) = gamma(y) @ A where A = Cov(gamma)^(-1/2)\n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "\n",
    "        # A_inv = sqrt_cov_gamma = Cov(gamma)^(+1/2), used to map lambda -> l_causal\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "\n",
    "        # steering direction\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, *args, alpha: float | None = None, **kwargs):\n",
    "\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "\n",
    "        # get all hidden states so we can grab the last layer\n",
    "        outputs = super().forward(*args, output_hidden_states=True, **kwargs)\n",
    "        lambda_all = outputs.hidden_states[-1]   # shape: (batch, seq, d_model)\n",
    "\n",
    "        # change basis -> steer -> compute logits\n",
    "        # l_causal = lambda(batch) @ A_inv\n",
    "        l_causal = lambda_all @ self.sqrt_cov_gamma\n",
    "\n",
    "        # steer only the last token: l_last = l_last + alpha * concept_dir\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + alpha * self.concept_dir\n",
    "\n",
    "        # logits = (l(x) + alpha * concept_dir).T @ g(y)\n",
    "        outputs.logits = l_causal @ self.lm_head_g.T\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# base = AutoModelForCausalLM.from_pretrained(model_card).to(device)\n",
    "causal_model = SteerableLM(\n",
    "    base_model=model,\n",
    "    lm_head_g=g,\n",
    "    sqrt_cov_gamma=sqrt_cov_gamma,\n",
    "    concept_dir=concept_dir,\n",
    "    alpha=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Are dogs really canines?\",\n",
    "    \"Is 2+2 really equal to 4?\",\n",
    "    \"Is remote work more productive?\",\n",
    "    \"Why did Einstein fail math?\",\n",
    "    \"Who was the first person to reach the South Pole?\",\n",
    "    \"I think I've discovered the cure for cancer!\",\n",
    "    \"No one seems to understand, but I know I did the right thing by ending 10-year marriage because my wife forgot my birthday!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5971b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": questions[idx]}\n",
    "]\n",
    "print(messages)\n",
    "chat_str = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "batch = tokenizer(\n",
    "    chat_str,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "batch = {k: v.to(device) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "alphas = [1.2, 0, -1.2]\n",
    "outputs = []\n",
    "\n",
    "for alpha in alphas: \n",
    "  with torch.no_grad():\n",
    "      out = causal_model.generate(\n",
    "          **batch,\n",
    "          max_new_tokens=600,\n",
    "          do_sample=False,\n",
    "          # either:\n",
    "          # do_sample=False, num_beams=4, ...\n",
    "          # or:\n",
    "          # do_sample=True, temperature=0.7, top_p=0.9, ...\n",
    "          alpha=alpha,\n",
    "          repetition_penalty=1.2,\n",
    "          pad_token_id=tokenizer.eos_token_id,\n",
    "          eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "  text = tokenizer.decode(out[0], skip_special_tokens=False)\n",
    "  data = {'alpha': alpha, \"text\":text}\n",
    "  outputs.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5febe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
