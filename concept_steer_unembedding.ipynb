{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8620fd59",
   "metadata": {},
   "source": [
    "### Load in LM for testing\n",
    "- interstingly gpt2-medium throws nan's but script seems to work well for small and large\n",
    "- each lm_head has different dims so small is 768, medium 1024, large 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1c0835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cache_dir = (Path.cwd() / \"models\").resolve()\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    # else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a96d53",
   "metadata": {},
   "source": [
    "### Change basis for lm_head\n",
    "- [linear_rep_geometry\n",
    "/store_matrices.py](https://github.com/KihoPark/linear_rep_geometry/blob/main/store_matrices.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05d815b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "torch.Size([50257, 1280])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.lm_head.weight.detach()\n",
    "W, d = gamma.shape\n",
    "gamma_bar = torch.mean(gamma, dim=0)\n",
    "centered_gamma = gamma - gamma_bar\n",
    "\n",
    "### compute Cov(gamma) and tranform gamma to g ###\n",
    "cov_gamma = centered_gamma.T @ centered_gamma / W\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_gamma)\n",
    "\n",
    "inv_sqrt_cov_gamma = eigenvectors @ torch.diag(1/torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "sqrt_cov_gamma = eigenvectors @ torch.diag(torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# gamma is our original head and inv_sqrt_cov_gamma puts us in a causal basis\n",
    "g = gamma @ inv_sqrt_cov_gamma\n",
    "\n",
    "# maybe i confused but A_inv = sqrt_cov_gamma and A = inv_sqrt_cov_gamma for \n",
    "# l(x).T @ g(y)\n",
    "# where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A (referencing paper eq and presentation eq on youtube)\n",
    "print(model.config.hidden_size)\n",
    "print(g.size())\n",
    "\n",
    "# g = g.float()\n",
    "# inv_sqrt_cov_gamma = inv_sqrt_cov_gamma.float()\n",
    "# sqrt_cov_gamma = sqrt_cov_gamma.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5975acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenval min: 7.621865370310843e-05\n",
      "Eigenval max: 0.08969025313854218\n",
      "Max amplification (1/sqrt(min)): 114.5\n",
      "\n",
      "gamma min: -0.5182147026062012\n",
      "gamma max: 0.4273260533809662\n",
      "\n",
      "gamma @ inv_sqrt_cov_gamma min: -8.831315994262695\n",
      "gamma @ inv_sqrt_cov_gamma max: 8.074169158935547\n",
      "\n",
      "gamma dtype: torch.float32\n",
      "g dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "eigenval_min_max = f\"Eigenval min: {eigenvalues.min()}\\nEigenval max: {eigenvalues.max()}\"\n",
    "max_amp = f\"Max amplification (1/sqrt(min)): {1 / torch.sqrt(eigenvalues.min()).item():.1f}\\n\"\n",
    "gamma_min_max = f\"gamma min: {gamma.min()}\\ngamma max: {gamma.max()}\\n\"\n",
    "g_min_max = f\"gamma @ inv_sqrt_cov_gamma min: {g.min()}\\ngamma @ inv_sqrt_cov_gamma max: {g.max()}\\n\"\n",
    "\n",
    "print(eigenval_min_max)\n",
    "print(max_amp)\n",
    "print(gamma_min_max)\n",
    "print(g_min_max)\n",
    "print(f\"gamma dtype: {gamma.dtype}\")\n",
    "print(f\"g dtype: {g.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dfd26",
   "metadata": {},
   "source": [
    "### Define data for training probe and inference tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d11b2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "concept_df = pd.read_json('gendered_pairs/data.json')\n",
    "concept_df = concept_df[concept_df.get('skip', False) != True] # skipping pronoun contrastive pairs\n",
    "male_ex = concept_df['male_word'].to_list()\n",
    "female_ex = concept_df['female_word'].to_list()\n",
    "\n",
    "inference_prompts = [\n",
    "    \"Long live the\",\n",
    "    \"The lion is the\",\n",
    "    \"In the hierarchy of medieval society, the highest rank was the\",\n",
    "    \"Arthur was a legendary\",\n",
    "    \"He was known as the warrior\",\n",
    "    \"In a monarchy, the ruler is usually a\",\n",
    "    \"He sat on the throne, the\",\n",
    "    \"A sovereign ruler in a monarchy is often a\",\n",
    "    \"His domain was vast, for he was a\",\n",
    "    \"The lion, in many cultures, is considered the\",\n",
    "    \"He wore a crown, signifying he was the\",\n",
    "    \"A male sovereign who reigns over a kingdom is a\",\n",
    "    \"Every kingdom has its ruler, typically a\",\n",
    "    \"The prince matured and eventually became the\",\n",
    "    \"In the deck of cards, alongside the queen is the\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed2b70",
   "metadata": {},
   "source": [
    "### get concept direction\n",
    "- just getting the avg of difference vector between gamma(Y(0)) - gamma(Y(1))\n",
    "- gut says logistic reg probe is better because error/noise can be better quantified \n",
    "    - plus can determine alpha that acheives desired prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "72d94cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "m_inputs = tokenizer(male_ex, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "f_inputs = tokenizer(female_ex, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "m_exclude = m_inputs.attention_mask.sum(dim=1) > 1\n",
    "f_exclude = f_inputs.attention_mask.sum(dim=1) > 1\n",
    "valid_mask = torch.stack([m_exclude,f_exclude]).T.sum(dim=1) == 0\n",
    "\n",
    "m_ready = m_inputs.input_ids[valid_mask][:,0]\n",
    "f_ready = f_inputs.input_ids[valid_mask][:,0]\n",
    "\n",
    "print(m_ready.size())\n",
    "gamma_bar_W = (gamma[m_ready] - gamma[f_ready]).mean(dim=0)\n",
    "\n",
    "# can do it this way or (g[m_ready] - g[f_ready]).mean(dim=0)\n",
    "concept_dir = (gamma[m_ready] - gamma[f_ready]).mean(dim=0) # or (g[m_ready] - g[f_ready]).mean(dim=0)\n",
    "concept_dir = concept_dir @ inv_sqrt_cov_gamma\n",
    "concept_dir /= concept_dir.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be313a",
   "metadata": {},
   "source": [
    "### Inference steering\n",
    "- assuming i just keep adding gender dir at each decoding step\n",
    "- this could be simpler to put in logit processor so we can use model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5ff19971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'In the hierarchy of medieval society, the highest rank was the'\n",
      "\n",
      "| alpha = 1       | alpha = 0      | alpha = -1       |\n",
      "|-----------------|----------------|------------------|\n",
      "| man (-1.35)     | knight (-3.11) | woman (-1.83)    |\n",
      "| knight (-2.03)  | \" (-3.14)      | lady (-2.33)     |\n",
      "| lord (-2.78)    | bishop (-3.22) | woman (-3.36)    |\n",
      "| Prince (-3.13)  | d (-3.44)      | female (-3.39)   |\n",
      "| prince (-3.53)  | bar (-3.70)    | wife (-3.44)     |\n",
      "| \" (-3.70)       | kn (-3.74)     | nun (-3.79)      |\n",
      "| king (-3.71)    | monk (-3.86)   | princess (-3.82) |\n",
      "| bishop (-4.15)  | noble (-3.89)  | kn (-3.85)       |\n",
      "| husband (-4.51) | king (-3.90)   | d (-3.93)        |\n",
      "| monk (-4.55)    | lord (-4.04)   | she (-3.93)      |\n",
      "| noble (-4.59)   | priest (-4.19) | ear (-3.96)      |\n",
      "| son (-4.60)     | ' (-4.26)      | queen (-4.04)    |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "prompt = inference_prompts[2]\n",
    "prompt_enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "# what are sensible alphas here e.g. values that acheive some target prob for that class?\n",
    "alphas = [1, 0, -1]\n",
    "k = 12\n",
    "cols = []\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    with torch.no_grad():\n",
    "        # transform, steer, and get causal inner-product between embedding and unembedding\n",
    "        outputs = model(**prompt_enc, output_hidden_states=True)\n",
    "        last_token_idx = (prompt_enc.attention_mask.sum(dim=1) - 1).item()\n",
    "\n",
    "        # (l(x) + alpha * gender_norm).T @ g(y)\n",
    "        # where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A \n",
    "        # Note: Code seems to swap A_inv with A e.g. \n",
    "        # it applies A_inv to gamma (lm_head) instead of lambda (embeddings) like equation suggests\n",
    "\n",
    "        lambda_x = outputs.hidden_states[-1][:, last_token_idx, :] # last token emb but calling it lambda for paper consistence\n",
    "        l_causal = lambda_x @ sqrt_cov_gamma\n",
    "        l_steered = l_causal + alpha * concept_dir\n",
    "\n",
    "        causal_logits = l_steered @ g.T\n",
    "        causal_log_probs = torch.log_softmax(causal_logits, dim=-1)\n",
    "        topk = torch.topk(causal_log_probs, k)\n",
    "\n",
    "        tokens = [tokenizer.decode([idx.item()]) for idx in topk.indices[0]]\n",
    "        log_probs = topk.values[0].tolist()\n",
    "        cols.append([f\"{tok} ({lp:.2f})\" for tok, lp in zip(tokens, log_probs)])\n",
    "\n",
    "rows = list(zip(*cols))\n",
    "table = tabulate(rows, headers=[f\"alpha = {a}\" for a in alphas], tablefmt=\"github\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dce0c8",
   "metadata": {},
   "source": [
    "### custom lm_head \n",
    "if the code is right could easily create a logits processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8b406ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "class SteerableGPT2(GPT2LMHeadModel):\n",
    "    def __init__(self, base_model, lm_head_g, sqrt_cov_gamma, concept_dir, alpha: float = 0.0):\n",
    "        super().__init__(base_model.config)\n",
    "        # reuse base model's transformer + original head\n",
    "        self.transformer = base_model.transformer\n",
    "        self.lm_head= base_model.lm_head\n",
    "\n",
    "        # g(y) = gamma(y) @ A where A = Cov(gamma)^(-1/2) \n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "\n",
    "        # A_inv = sqrt_cov_gamma = Cov(gamma)^(+1/2), used to map lambda -> l_causal\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "\n",
    "        # steering direction\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        # get all hidden states so we can grab the last layer\n",
    "        outputs = super().forward(*args, output_hidden_states=True, **kwargs)\n",
    "        lambda_all = outputs.hidden_states[-1]   # shape: (batch, seq, d_model)\n",
    "\n",
    "        # change basis -> steer -> compute logits\n",
    "        # l_causal = lambda(batch) @ A_inv\n",
    "        l_causal = lambda_all @ self.sqrt_cov_gamma\n",
    "\n",
    "        # steer only the last token: l_last = l_last + alpha * concept_dir\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + self.alpha * self.concept_dir\n",
    "\n",
    "        # logits = (l(x) + alpha * concept_dir).T @ g(y)\n",
    "        outputs.logits = l_causal @ self.lm_head_g.T\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)\n",
    "causal_model = SteerableGPT2(\n",
    "    base_model=base,\n",
    "    lm_head_g=g,\n",
    "    sqrt_cov_gamma=sqrt_cov_gamma,\n",
    "    concept_dir=concept_dir, \n",
    "    alpha=-1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d2169bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:       The lion, in many cultures, is considered the\n",
      "Continuation: The lion, in many cultures, is considered the mother of women. Women are mothers to female lions and females have a womanly role within their families.\"\n",
      "\n",
      "Women's\n"
     ]
    }
   ],
   "source": [
    "prompt = inference_prompts[9]\n",
    "prompt_enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "output = causal_model.generate(\n",
    "    input_ids=prompt_enc.input_ids,\n",
    "    attention_mask=prompt_enc.attention_mask,\n",
    "    max_new_tokens=25,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.85,\n",
    "    repetition_penalty=1.5,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"Prompt:       {prompt}\")\n",
    "print(f\"Continuation: {''.join(tokenizer.batch_decode(output))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e758731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
