{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8620fd59",
   "metadata": {},
   "source": [
    "### Load in LM for testing\n",
    "- interstingly gpt2-medium throws nan's but script seems to work well for small and large\n",
    "- each lm_head has different dims so small is 768, medium 1024, large 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c0835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3840, nx=1280)\n",
       "          (c_proj): Conv1D(nf=1280, nx=1280)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=5120, nx=1280)\n",
       "          (c_proj): Conv1D(nf=1280, nx=5120)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cache_dir = (Path.cwd() / \"models\").resolve()\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    # else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a96d53",
   "metadata": {},
   "source": [
    "### Change basis for lm_head\n",
    "- [linear_rep_geometry\n",
    "/store_matrices.py](https://github.com/KihoPark/linear_rep_geometry/blob/main/store_matrices.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d815b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "torch.Size([50257, 1280])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.lm_head.weight.detach()\n",
    "W, d = gamma.shape\n",
    "gamma_bar = torch.mean(gamma, dim=0)\n",
    "centered_gamma = gamma - gamma_bar\n",
    "\n",
    "### compute Cov(gamma) and tranform gamma to g ###\n",
    "cov_gamma = centered_gamma.T @ centered_gamma / W\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_gamma)\n",
    "\n",
    "inv_sqrt_cov_gamma = eigenvectors @ torch.diag(1/torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "sqrt_cov_gamma = eigenvectors @ torch.diag(torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# gamma is our original head and inv_sqrt_cov_gamma puts us in a causal basis\n",
    "g = gamma @ inv_sqrt_cov_gamma\n",
    "\n",
    "# maybe i confused but A_inv = sqrt_cov_gamma and A = inv_sqrt_cov_gamma for \n",
    "# l(x).T @ g(y)\n",
    "# where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A (referencing paper eq and presentation eq on youtube)\n",
    "print(model.config.hidden_size)\n",
    "print(g.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dfd26",
   "metadata": {},
   "source": [
    "### Define data for training probe and inference tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d11b2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs = {\n",
    "    \"he is the king\": \"she is the queen\",\n",
    "    \"he is a man\": \"she is a woman\",\n",
    "    \"he is the prince\": \"she is the princess\",\n",
    "    \"he is an actor\": \"she is an actress\",\n",
    "    \"he is my brother\": \"she is my sister\",\n",
    "    \"he is my father\": \"she is my mother\",\n",
    "    \"he is my son\": \"she is my daughter\",\n",
    "    \"he is my uncle\": \"she is my aunt\",\n",
    "    \"he is my husband\": \"she is my wife\",\n",
    "    \"he is my grandfather\": \"she is my grandmother\",\n",
    "    \"he is my nephew\": \"she is my niece\",\n",
    "    \"he was a gentleman\": \"she was a lady\",\n",
    "    \"the boy ran home\": \"the girl ran home\",\n",
    "    \"Mr. Smith arrived\": \"Ms. Smith arrived\",\n",
    "    \"the chairman spoke\": \"the chairwoman spoke\",\n",
    "    \"he is a waiter\": \"she is a waitress\",\n",
    "    \"the spokesman said\": \"the spokeswoman said\",\n",
    "    \"he is a hero\": \"she is a heroine\",\n",
    "    \"the landlord agreed\": \"the landlady agreed\",\n",
    "    \"he is a duke\": \"she is a duchess\",\n",
    "}\n",
    "\n",
    "inference_prompts = [\n",
    "    \"Long live the\",\n",
    "    \"The lion is the\",\n",
    "    \"In the hierarchy of medieval society, the highest rank was the\",\n",
    "    \"Arthur was a legendary\",\n",
    "    \"He was known as the warrior\",\n",
    "    \"In a monarchy, the ruler is usually a\",\n",
    "    \"He sat on the throne, the\",\n",
    "    \"A sovereign ruler in a monarchy is often a\",\n",
    "    \"His domain was vast, for he was a\",\n",
    "    \"The lion, in many cultures, is considered the\",\n",
    "    \"He wore a crown, signifying he was the\",\n",
    "    \"A male sovereign who reigns over a kingdom is a\",\n",
    "    \"Every kingdom has its ruler, typically a\",\n",
    "    \"The prince matured and eventually became the\",\n",
    "    \"In the deck of cards, alongside the queen is the\"\n",
    "]\n",
    "\n",
    "male_ex = list(gender_pairs.keys())\n",
    "female_ex = list(gender_pairs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c951dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_inputs = tokenizer(male_ex, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "f_inputs = tokenizer(female_ex, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model(**m_inputs, output_hidden_states=True)\n",
    "    out2 = model(**f_inputs, output_hidden_states=True)\n",
    "    last_hidden_m, last_hidden_f = out1.hidden_states[-1], out2.hidden_states[-1]\n",
    "    # we want to pool embeddings for each token in sequence so we have full sequence representation\n",
    "    m_emb, f_emb = last_hidden_m.mean(dim=1), last_hidden_f.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf1282",
   "metadata": {},
   "source": [
    "### train probe\n",
    "- could probably just get difference vec would like to ask about this tbh\n",
    "- gut says probe is better because error/noise can be better quantified \n",
    "    - plus can determine alpha that acheives desired prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e54baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# make sure probe is trained in right representation space \n",
    "# since we will apply it to l(x) later where l(x) = lambda(x) @ A_inv\n",
    "X = torch.cat([m_emb, f_emb], dim=0) @ sqrt_cov_gamma\n",
    "\n",
    "y = torch.cat([\n",
    "    torch.ones(len(male_ex)), \n",
    "    torch.zeros(len(female_ex))\n",
    "    ])\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    penalty=None,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "clf.fit(X.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "concept_dir = torch.tensor(clf.coef_[0], dtype=torch.float32, device=device)\n",
    "concept_dir_norm = concept_dir / concept_dir.norm()\n",
    "\n",
    "# simple difference\n",
    "# gender_dir = (m_emb - f_emb).mean(dim=0)\n",
    "# gender_dir = gender_dir # @ sqrt_cov_gamma\n",
    "# gender_norm = gender_dir/gender_dir.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be313a",
   "metadata": {},
   "source": [
    "### Inference steering\n",
    "- assuming i just keep adding gender dir at each decoding step\n",
    "- this could be simpler to put in logit processor so we can use model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ff19971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1280]) torch.Size([1280, 50257])\n",
      "Long live the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' queen', -1.2349622249603271),\n",
       " (' Queen', -1.879753828048706),\n",
       " (' woman', -3.449702501296997),\n",
       " (' goddess', -3.5900542736053467),\n",
       " (' lady', -3.802884817123413),\n",
       " (' mother', -3.927058458328247),\n",
       " (' Goddess', -4.263751029968262),\n",
       " (' princess', -4.353240013122559),\n",
       " (' feminist', -4.536863327026367),\n",
       " (' Empress', -4.895029067993164),\n",
       " (' girl', -5.392460823059082),\n",
       " (' women', -5.425230979919434),\n",
       " (' beautiful', -5.4550371170043945),\n",
       " (' Lady', -5.532746315002441),\n",
       " (' beauty', -5.558073997497559),\n",
       " (' brave', -5.569255828857422),\n",
       " (' bitch', -5.586803436279297),\n",
       " (' witch', -5.664284706115723),\n",
       " (' Princess', -5.787386894226074),\n",
       " (' king', -5.943777084350586)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = inference_prompts[0]\n",
    "prompt_enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "# what are sensible alphas here e.g. values that acheive some target prob for that class?\n",
    "alpha = -1.0\n",
    "\n",
    "# transform, steer, and get causal inner-product between embedding and unembedding\n",
    "with torch.no_grad():\n",
    "    outputs = model(**prompt_enc, output_hidden_states=True)\n",
    "    last_token_idx = (prompt_enc.attention_mask.sum(dim=1) - 1).item()\n",
    "\n",
    "    # (l(x) + alpha * gender_norm).T @ g(y)\n",
    "    # where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A \n",
    "    # Note: Code seems to swap A_inv with A e.g. \n",
    "    # it applies A_inv to gamma (lm_head) instead of lambda (embeddings) like equation suggests\n",
    "\n",
    "    lambda_x = outputs.hidden_states[-1][:, last_token_idx, :] # last token emb but calling it lambda for paper consistence\n",
    "    l_causal = lambda_x @ sqrt_cov_gamma\n",
    "    l_steered = l_causal + alpha * concept_dir_norm\n",
    "\n",
    "    print(l_steered.size(), g.T.size())\n",
    "    causal_logits = l_steered @ g.T\n",
    "    causal_log_probs = torch.log_softmax(causal_logits, dim=-1)\n",
    "    \n",
    "    topk = torch.topk(causal_log_probs, 20)\n",
    "    tokens = [tokenizer.decode([idx.item()]) for idx in topk.indices[0]]\n",
    "    log_probs = topk.values[0].tolist()\n",
    "    result = list(zip(tokens, log_probs))\n",
    "\n",
    "print(prompt)  \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dce0c8",
   "metadata": {},
   "source": [
    "### custom lm_head \n",
    "if the code is right could easily create a logits processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b406ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "class SteerableGPT2(GPT2LMHeadModel):\n",
    "    def __init__(self, base_model, lm_head_g, sqrt_cov_gamma, concept_dir, alpha: float = 0.0):\n",
    "        super().__init__(base_model.config)\n",
    "        # reuse base model's transformer + original head\n",
    "        self.transformer = base_model.transformer\n",
    "        self.lm_head= base_model.lm_head\n",
    "\n",
    "        # g(y) = gamma(y) @ A where A = Cov(gamma)^(-1/2) \n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "\n",
    "        # A_inv = sqrt_cov_gamma = Cov(gamma)^(+1/2), used to map lambda -> l_causal\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "\n",
    "        # steering direction\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        # get all hidden states so we can grab the last layer\n",
    "        outputs = super().forward(*args, output_hidden_states=True, **kwargs)\n",
    "        lambda_all = outputs.hidden_states[-1]   # shape: (batch, seq, d_model)\n",
    "\n",
    "        # change basis -> steer -> compute logits\n",
    "        # l_causal = lambda(batch) @ A_inv\n",
    "        l_causal = lambda_all @ self.sqrt_cov_gamma\n",
    "\n",
    "        # steer only the last token: l_last = l_last + alpha * concept_dir\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + self.alpha * self.concept_dir\n",
    "\n",
    "        # logits = (l(x) + alpha * concept_dir).T @ g(y)\n",
    "        outputs.logits = l_causal @ self.lm_head_g.T\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)\n",
    "\n",
    "causal_model = SteerableGPT2(\n",
    "    base_model=base,\n",
    "    lm_head_g=g,\n",
    "    sqrt_cov_gamma=sqrt_cov_gamma,\n",
    "    concept_dir=concept_dir_norm, \n",
    "    alpha=-0.7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2169bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:       The lion is the\n",
      "Continuation: The lion is the symbol of power and beauty, but it's also a metaphor for sexuality.\n",
      "\n",
      "\"I'm not saying that women are necessarily attracted to lions,\" she says. \"But I think we can see in our culture how beautiful they may be.\"<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = inference_prompts[1]\n",
    "prompt_enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "output = causal_model.generate(\n",
    "    # input_ids,\n",
    "    # max_new_tokens=30,\n",
    "    # no_repeat_ngram_size=3,\n",
    "    # do_sample=True,\n",
    "    # temperature=0.65,\n",
    "    # top_k=40,\n",
    "    # repetition_penalty=1.5,\n",
    "    # pad_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "    input_ids=prompt_enc.input_ids,\n",
    "    attention_mask=prompt_enc.attention_mask,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.85,\n",
    "    repetition_penalty=1.5,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f'Prompt:       {prompt}')\n",
    "print(f'Continuation: {''.join(tokenizer.batch_decode(output))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
