{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8620fd59",
   "metadata": {},
   "source": [
    "### Load in LM for testing\n",
    "- interstingly gpt2-medium throws nan's but script seems to work well for small and large\n",
    "- each lm_head has different dims so small is 768, medium 1024, large 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c0835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3840, nx=1280)\n",
       "          (c_proj): Conv1D(nf=1280, nx=1280)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=5120, nx=1280)\n",
       "          (c_proj): Conv1D(nf=1280, nx=5120)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cache_dir = (Path.cwd() / \"models\").resolve()\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    # else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a96d53",
   "metadata": {},
   "source": [
    "### Change basis for lm_head\n",
    "- [linear_rep_geometry\n",
    "/store_matrices.py](https://github.com/KihoPark/linear_rep_geometry/blob/main/store_matrices.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d815b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "torch.Size([50257, 1280])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.lm_head.weight.detach()\n",
    "W, d = gamma.shape\n",
    "gamma_bar = torch.mean(gamma, dim=0)\n",
    "centered_gamma = gamma - gamma_bar\n",
    "\n",
    "### compute Cov(gamma) and tranform gamma to g ###\n",
    "cov_gamma = centered_gamma.T @ centered_gamma / W\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_gamma)\n",
    "\n",
    "inv_sqrt_cov_gamma = eigenvectors @ torch.diag(1/torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "sqrt_cov_gamma = eigenvectors @ torch.diag(torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# gamma is our original head and inv_sqrt_cov_gamma puts us in a causal basis\n",
    "g = gamma @ inv_sqrt_cov_gamma\n",
    "\n",
    "# maybe i confused but A_inv = sqrt_cov_gamma and A = inv_sqrt_cov_gamma for \n",
    "# l(x).T @ g(y)\n",
    "# where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A (referencing paper eq and presentation eq on youtube)\n",
    "print(model.config.hidden_size)\n",
    "print(g.size())\n",
    "\n",
    "g = g.float()\n",
    "inv_sqrt_cov_gamma = inv_sqrt_cov_gamma.float()\n",
    "sqrt_cov_gamma = sqrt_cov_gamma.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5975acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenval min: 7.621865370310843e-05\n",
      "Eigenval max: 0.08969025313854218\n",
      "Max amplification (1/sqrt(min)): 114.5\n",
      "\n",
      "gamma min: -0.5182147026062012\n",
      "gamma max: 0.4273260533809662\n",
      "\n",
      "gamma @ inv_sqrt_cov_gamma min: -8.831315994262695\n",
      "gamma @ inv_sqrt_cov_gamma max: 8.074169158935547\n",
      "\n",
      "gamma dtype: torch.float32\n",
      "g dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "eigenval_min_max = f\"Eigenval min: {eigenvalues.min()}\\nEigenval max: {eigenvalues.max()}\"\n",
    "max_amp = f\"Max amplification (1/sqrt(min)): {1 / torch.sqrt(eigenvalues.min()).item():.1f}\\n\"\n",
    "gamma_min_max = f\"gamma min: {gamma.min()}\\ngamma max: {gamma.max()}\\n\"\n",
    "g_min_max = f\"gamma @ inv_sqrt_cov_gamma min: {g.min()}\\ngamma @ inv_sqrt_cov_gamma max: {g.max()}\\n\"\n",
    "\n",
    "print(eigenval_min_max)\n",
    "print(max_amp)\n",
    "print(gamma_min_max)\n",
    "print(g_min_max)\n",
    "print(f\"gamma dtype: {gamma.dtype}\")\n",
    "print(f\"g dtype: {g.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dfd26",
   "metadata": {},
   "source": [
    "### Define data for training probe and inference tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11b2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs = {\n",
    "    \"he is the king\": \"she is the queen\",\n",
    "    \"he is a man\": \"she is a woman\",\n",
    "    \"he is the prince\": \"she is the princess\",\n",
    "    \"he is an actor\": \"she is an actress\",\n",
    "    \"he is my brother\": \"she is my sister\",\n",
    "    \"he is my father\": \"she is my mother\",\n",
    "    \"he is my son\": \"she is my daughter\",\n",
    "    \"he is my uncle\": \"she is my aunt\",\n",
    "    \"he is my husband\": \"she is my wife\",\n",
    "    \"he is my grandfather\": \"she is my grandmother\",\n",
    "    \"he is my nephew\": \"she is my niece\",\n",
    "    \"he was a gentleman\": \"she was a lady\",\n",
    "    \"the boy ran home\": \"the girl ran home\",\n",
    "    \"Mr. Smith arrived\": \"Ms. Smith arrived\",\n",
    "    \"the chairman spoke\": \"the chairwoman spoke\",\n",
    "    \"he is a waiter\": \"she is a waitress\",\n",
    "    \"the spokesman said\": \"the spokeswoman said\",\n",
    "    \"he is a hero\": \"she is a heroine\",\n",
    "    \"the landlord agreed\": \"the landlady agreed\",\n",
    "    \"he is a duke\": \"she is a duchess\",\n",
    "}\n",
    "\n",
    "inference_prompts = [\n",
    "    \"Long live the\",\n",
    "    \"The lion is the\",\n",
    "    \"In the hierarchy of medieval society, the highest rank was the\",\n",
    "    \"Arthur was a legendary\",\n",
    "    \"He was known as the warrior\",\n",
    "    \"In a monarchy, the ruler is usually a\",\n",
    "    \"He sat on the throne, the\",\n",
    "    \"A sovereign ruler in a monarchy is often a\",\n",
    "    \"His domain was vast, for he was a\",\n",
    "    \"The lion, in many cultures, is considered the\",\n",
    "    \"He wore a crown, signifying he was the\",\n",
    "    \"A male sovereign who reigns over a kingdom is a\",\n",
    "    \"Every kingdom has its ruler, typically a\",\n",
    "    \"The prince matured and eventually became the\",\n",
    "    \"In the deck of cards, alongside the queen is the\"\n",
    "]\n",
    "\n",
    "male_ex = list(gender_pairs.keys())\n",
    "female_ex = list(gender_pairs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c951dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_inputs = tokenizer(male_ex, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "f_inputs = tokenizer(female_ex, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model(**m_inputs, output_hidden_states=True)\n",
    "    out2 = model(**f_inputs, output_hidden_states=True)\n",
    "    last_hidden_m, last_hidden_f = out1.hidden_states[-1], out2.hidden_states[-1]\n",
    "    # we want to pool embeddings for each token in sequence so we have full sequence representation\n",
    "    m_emb, f_emb = last_hidden_m.mean(dim=1), last_hidden_f.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf1282",
   "metadata": {},
   "source": [
    "### train probe\n",
    "- could probably just get difference vec would like to ask about this tbh\n",
    "- gut says probe is better because error/noise can be better quantified \n",
    "    - plus can determine alpha that acheives desired prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e54baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# make sure probe is trained in right representation space \n",
    "# since we will apply it to l(x) later where l(x) = lambda(x) @ A_inv\n",
    "X = torch.cat([m_emb, f_emb], dim=0) @ sqrt_cov_gamma\n",
    "\n",
    "y = torch.cat([\n",
    "    torch.ones(len(male_ex)), \n",
    "    torch.zeros(len(female_ex))\n",
    "    ])\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    penalty=None,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "clf.fit(X.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "concept_dir = torch.tensor(clf.coef_[0], dtype=torch.float32, device=device)\n",
    "concept_dir_norm = concept_dir / concept_dir.norm()\n",
    "\n",
    "# simple difference\n",
    "# gender_dir = (m_emb - f_emb).mean(dim=0)\n",
    "# gender_dir = gender_dir # @ sqrt_cov_gamma\n",
    "# gender_norm = gender_dir/gender_dir.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be313a",
   "metadata": {},
   "source": [
    "### Inference steering\n",
    "- assuming i just keep adding gender dir at each decoding step\n",
    "- this could be simpler to put in logit processor so we can use model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff19971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'In the hierarchy of medieval society, the highest rank was the'\n",
      "\n",
      "| alpha = 1.5        | alpha = 0      | alpha = -1.5     |\n",
      "|--------------------|----------------|------------------|\n",
      "| king (-1.33)       | knight (-3.11) | nun (-1.61)      |\n",
      "| King (-2.75)       | \" (-3.14)      | lady (-2.05)     |\n",
      "| monk (-2.92)       | bishop (-3.22) | she (-2.95)      |\n",
      "| lord (-3.23)       | d (-3.44)      | d (-2.99)        |\n",
      "| bishop (-3.35)     | bar (-3.70)    | kn (-3.81)       |\n",
      "| \" (-3.49)          | kn (-3.74)     | princess (-3.94) |\n",
      "| prince (-3.87)     | monk (-3.86)   | woman (-3.95)    |\n",
      "| knight (-4.26)     | noble (-3.89)  | convent (-3.96)  |\n",
      "| son (-4.26)        | king (-3.90)   | v (-3.97)        |\n",
      "| Archbishop (-4.27) | lord (-4.04)   | herself (-4.17)  |\n",
      "| priest (-4.37)     | priest (-4.19) | widow (-4.28)    |\n",
      "| man (-4.44)        | ' (-4.26)      | queen (-4.30)    |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "prompt = inference_prompts[2]\n",
    "prompt_enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "# what are sensible alphas here e.g. values that acheive some target prob for that class?\n",
    "alphas = [1.5, 0, -1.5]\n",
    "k = 12\n",
    "cols = []\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "\n",
    "for alpha in alphas:\n",
    "    with torch.no_grad():\n",
    "        # transform, steer, and get causal inner-product between embedding and unembedding\n",
    "        outputs = model(**prompt_enc, output_hidden_states=True)\n",
    "        last_token_idx = (prompt_enc.attention_mask.sum(dim=1) - 1).item()\n",
    "\n",
    "        # (l(x) + alpha * gender_norm).T @ g(y)\n",
    "        # where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A \n",
    "        # Note: Code seems to swap A_inv with A e.g. \n",
    "        # it applies A_inv to gamma (lm_head) instead of lambda (embeddings) like equation suggests\n",
    "\n",
    "        lambda_x = outputs.hidden_states[-1][:, last_token_idx, :] # last token emb but calling it lambda for paper consistence\n",
    "        l_causal = lambda_x @ sqrt_cov_gamma\n",
    "        l_steered = l_causal + alpha * concept_dir_norm\n",
    "\n",
    "        causal_logits = l_steered @ g.T\n",
    "        causal_log_probs = torch.log_softmax(causal_logits, dim=-1)\n",
    "        topk = torch.topk(causal_log_probs, k)\n",
    "\n",
    "        tokens = [tokenizer.decode([idx.item()]) for idx in topk.indices[0]]\n",
    "        log_probs = topk.values[0].tolist()\n",
    "        cols.append([f\"{tok} ({lp:.2f})\" for tok, lp in zip(tokens, log_probs)])\n",
    "\n",
    "rows = list(zip(*cols))\n",
    "table = tabulate(rows, headers=[f\"alpha = {a}\" for a in alphas], tablefmt=\"github\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dce0c8",
   "metadata": {},
   "source": [
    "### custom lm_head \n",
    "if the code is right could easily create a logits processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b406ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "class SteerableGPT2(GPT2LMHeadModel):\n",
    "    def __init__(self, base_model, lm_head_g, sqrt_cov_gamma, concept_dir, alpha: float = 0.0):\n",
    "        super().__init__(base_model.config)\n",
    "        # reuse base model's transformer + original head\n",
    "        self.transformer = base_model.transformer\n",
    "        self.lm_head= base_model.lm_head\n",
    "\n",
    "        # g(y) = gamma(y) @ A where A = Cov(gamma)^(-1/2) \n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "\n",
    "        # A_inv = sqrt_cov_gamma = Cov(gamma)^(+1/2), used to map lambda -> l_causal\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "\n",
    "        # steering direction\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        # get all hidden states so we can grab the last layer\n",
    "        outputs = super().forward(*args, output_hidden_states=True, **kwargs)\n",
    "        lambda_all = outputs.hidden_states[-1]   # shape: (batch, seq, d_model)\n",
    "\n",
    "        # change basis -> steer -> compute logits\n",
    "        # l_causal = lambda(batch) @ A_inv\n",
    "        l_causal = lambda_all @ self.sqrt_cov_gamma\n",
    "\n",
    "        # steer only the last token: l_last = l_last + alpha * concept_dir\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + self.alpha * self.concept_dir\n",
    "\n",
    "        # logits = (l(x) + alpha * concept_dir).T @ g(y)\n",
    "        outputs.logits = l_causal @ self.lm_head_g.T\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\").to(device)\n",
    "causal_model = SteerableGPT2(\n",
    "    base_model=base,\n",
    "    lm_head_g=g,\n",
    "    sqrt_cov_gamma=sqrt_cov_gamma,\n",
    "    concept_dir=concept_dir_norm, \n",
    "    alpha=-1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2169bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:       The lion, in many cultures, is considered the\n",
      "Continuation: The lion, in many cultures, is considered the symbol of beauty and fertility. The woman herself may be beautiful or she might have her breasts filled with milk to make herself pregnant\n"
     ]
    }
   ],
   "source": [
    "prompt = inference_prompts[9]\n",
    "prompt_enc = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "output = causal_model.generate(\n",
    "    input_ids=prompt_enc.input_ids,\n",
    "    attention_mask=prompt_enc.attention_mask,\n",
    "    max_new_tokens=25,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.85,\n",
    "    repetition_penalty=1.5,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"Prompt:       {prompt}\")\n",
    "print(f\"Continuation: {''.join(tokenizer.batch_decode(output))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
